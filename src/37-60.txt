vtune: Warning: To enable hardware event-based sampling, VTune Profiler has disabled the NMI watchdog timer. The watchdog timer will be re-enabled after collection completes.
vtune: Collection started. To stop the collection, either press CTRL-C or enter from another console window: vtune -r /home/brandon/jit-int16-matvec/r023ue -command stop.

        ---------- 

     1000000000 iterations, (16x64) * (64x1)
MKL JIT cgemm: 0.0000000000 µs per iteration
 my JIT int16: 0.0850534741 µs per iteration
  0.00x MKL JIT cgemm
---------------------------------

matvec: ./src/jit_int16_matvec.cpp:412: void benchDimensions(long int, long int, long int, bool): Assertion `vectorsEqual((float*)res, (int16_t*)res16, m)' failed.
vtune: Collection stopped.
vtune: Using result path `/home/brandon/jit-int16-matvec/r023ue'
vtune: Executing actions 20 % Resolving information for dangling locations     
vtune: Warning: Cannot locate file `vmlinux'.
vtune: Executing actions 21 % Resolving information for `matvec'               
vtune: Warning: Function and source-level analysis for the Linux kernel will not be possible since neither debug version of the kernel nor kernel symbol tables are found. See the Enabling Linux Kernel Analysis topic in the product online help for instructions.
vtune: Executing actions 75 % Generating a report                              Elapsed Time: 85.138s
    Clockticks: 282,308,000,000
    Instructions Retired: 420,147,000,000
    CPI Rate: 0.672
    MUX Reliability: 0.998
    Retiring: 37.4% of Pipeline Slots
        General Retirement: 37.4% of Pipeline Slots
            FP Arithmetic: 0.0% of uOps
                FP x87: 0.0% of uOps
                FP Scalar: 0.0% of uOps
                FP Vector: 0.0% of uOps
            Other: 100.0% of uOps
        Microcode Sequencer: 0.0% of Pipeline Slots
            Assists: 0.0% of Pipeline Slots
    Front-End Bound: 3.3% of Pipeline Slots
        Front-End Latency: 0.0% of Pipeline Slots
            ICache Misses: 0.0% of Clockticks
            ITLB Overhead: 0.0% of Clockticks
            Branch Resteers: 0.0% of Clockticks
                Mispredicts Resteers: 0.0% of Clockticks
                Clears Resteers: 0.0% of Clockticks
                Unknown Branches: 0.0% of Clockticks
            DSB Switches: 0.8% of Clockticks
            Length Changing Prefixes: 0.0% of Clockticks
            MS Switches: 0.0% of Clockticks
        Front-End Bandwidth: 3.3% of Pipeline Slots
            Front-End Bandwidth MITE: 19.8% of Clockticks
            Front-End Bandwidth DSB: 0.4% of Clockticks
            Front-End Bandwidth LSD: 0.0% of Clockticks
            (Info) DSB Coverage: 24.7%
            (Info) LSD Coverage: 0.0%
    Bad Speculation: 0.1% of Pipeline Slots
        Branch Mispredict: 0.0% of Pipeline Slots
        Machine Clears: 0.1% of Pipeline Slots
    Back-End Bound: 59.3% of Pipeline Slots
     | A significant portion of pipeline slots are remaining empty. When
     | operations take too long in the back-end, they introduce bubbles in the
     | pipeline that ultimately cause fewer pipeline slots containing useful
     | work to be retired per cycle than the machine is capable to support. This
     | opportunity cost results in slower execution. Long-latency operations
     | like divides and memory operations can cause this, as can too many
     | operations being directed to a single execution port (for example, more
     | multiply operations arriving in the back-end per cycle than the execution
     | unit can support).
     |
        Memory Bound: 17.9% of Pipeline Slots
         | The metric value is high. This can indicate that the significant
         | fraction of execution pipeline slots could be stalled due to demand
         | memory load and stores. Use Memory Access analysis to have the metric
         | breakdown by memory hierarchy, memory bandwidth information,
         | correlation by memory objects.
         |
            L1 Bound: 18.5% of Clockticks
             | This metric shows how often machine was stalled without missing
             | the L1 data cache. The L1 cache typically has the shortest
             | latency. However, in certain cases like loads blocked on older
             | stores, a load might suffer a high latency even though it is
             | being satisfied by the L1. Note that this metric value may be
             | highlighted due to DTLB Overhead or Cycles of 1 Port Utilized
             | issues.
             |
                DTLB Overhead: 0.0% of Clockticks
                    Load STLB Hit: 0.0% of Clockticks
                    Load STLB Miss: 0.0% of Clockticks
                Loads Blocked by Store Forwarding: 0.0% of Clockticks
                Lock Latency: 0.0% of Clockticks
                Split Loads: 0.0% of Clockticks
                4K Aliasing: 0.3% of Clockticks
                FB Full: 0.0% of Clockticks
            L2 Bound: 0.0% of Clockticks
            L3 Bound: 0.0% of Clockticks
                Contested Accesses: 0.0% of Clockticks
                Data Sharing: 0.0% of Clockticks
                L3 Latency: 0.0% of Clockticks
                SQ Full: 0.0% of Clockticks
            DRAM Bound: 0.0% of Clockticks
                Memory Bandwidth: 0.0% of Clockticks
                Memory Latency: 0.0% of Clockticks
                    Local DRAM: 0.0% of Clockticks
                    Remote DRAM: 0.0% of Clockticks
                    Remote Cache: 0.0% of Clockticks
            Store Bound: 0.0% of Clockticks
                Store Latency: 0.0% of Clockticks
                False Sharing: 0.0% of Clockticks
                Split Stores: 1.4% of Clockticks
                DTLB Store Overhead: 0.0% of Clockticks
                    Store STLB Hit: 0.0% of Clockticks
                    Store STLB Hit: 0.0% of Clockticks
        Core Bound: 41.3% of Pipeline Slots
         | This metric represents how much Core non-memory issues were of a
         | bottleneck. Shortage in hardware compute resources, or dependencies
         | software's instructions are both categorized under Core Bound. Hence
         | it may indicate the machine ran out of an OOO resources, certain
         | execution units are overloaded or dependencies in program's data- or
         | instruction- flow are limiting the performance (e.g. FP-chained long-
         | latency arithmetic operations).
         |
            Divider: 0.0% of Clockticks
            Port Utilization: 42.7% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to Core
             | non-divider-related issues.
             | 
             | Tips: Use vectorization to reduce pressure on the execution ports
             | as multiple elements are calculated with same uOp.
             |
                Cycles of 0 Ports Utilized: 9.3% of Clockticks
                    Serializing Operations: 0.0% of Clockticks
                        Slow Pause: 0.0% of Clockticks
                Cycles of 1 Port Utilized: 17.5% of Clockticks
                 | This metric represents cycles fraction where the CPU executed
                 | total of 1 uop per cycle on all execution ports. This can be
                 | due to heavy data-dependency among software instructions, or
                 | oversubscribing a particular hardware resource. In some other
                 | cases with high 1_Port_Utilized and L1 Bound, this metric can
                 | point to L1 data-cache latency bottleneck that may not
                 | necessarily manifest with complete execution starvation (due
                 | to the short L1 latency e.g. walking a linked list) - looking
                 | at the assembly can be helpful. Note that this metric value
                 | may be highlighted due to L1 Bound issue.
                 |
                Cycles of 2 Ports Utilized: 13.3% of Clockticks
                Cycles of 3+ Ports Utilized: 9.9% of Clockticks
                    ALU Operation Utilization: 18.8% of Clockticks
                        Port 0: 32.8% of Clockticks
                        Port 1: 2.3% of Clockticks
                        Port 5: 37.6% of Clockticks
                        Port 6: 2.4% of Clockticks
                    Load Operation Utilization: 36.4% of Clockticks
                        Port 2: 36.7% of Clockticks
                        Port 3: 36.9% of Clockticks
                    Store Operation Utilization: 2.4% of Clockticks
                        Port 4: 2.4% of Clockticks
                        Port 7: 1.7% of Clockticks
                Vector Capacity Usage (FPU): 0.0%
    Average CPU Frequency: 3.346 GHz 
    Total Thread Count: 1
    Paused Time: 0s
Effective Physical Core Utilization: 2.8% (0.991 out of 36)
 | The metric value is low, which may signal a poor physical CPU cores
 | utilization caused by:
 |     - load imbalance
 |     - threading runtime overhead
 |     - contended synchronization
 |     - thread/process underutilization
 |     - incorrect affinity that utilizes logical cores instead of physical
 |       cores
 | Explore sub-metrics to estimate the efficiency of MPI and OpenMP parallelism
 | or run the Locks and Waits analysis to identify parallel bottlenecks for
 | other parallel runtimes.
 |
    Effective Logical Core Utilization: 1.4% (0.991 out of 72)
     | The metric value is low, which may signal a poor logical CPU cores
     | utilization. Consider improving physical core utilization as the first
     | step and then look at opportunities to utilize logical cores, which in
     | some cases can improve processor throughput and overall performance of
     | multi-threaded applications.
     |
Collection and Platform Info
    Application Command Line: ./matvec 
    User Name: brandon
    Operating System: 4.15.0-112-generic NAME="Ubuntu" VERSION="18.04.4 LTS (Bionic Beaver)" ID=ubuntu ID_LIKE=debian PRETTY_NAME="Ubuntu 18.04.4 LTS" VERSION_ID="18.04" HOME_URL="https://www.ubuntu.com/" SUPPORT_URL="https://help.ubuntu.com/" BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/" PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy" VERSION_CODENAME=bionic UBUNTU_CODENAME=bionic
    Computer Name: ecl-mem-01
    Result Size: 206 MB 
    Collection start time: 18:07:52 03/08/2020 UTC
    Collection stop time: 18:09:18 03/08/2020 UTC
    Collector Type: Event-based sampling driver
    CPU
        Name: Intel(R) Xeon(R) Processor code named Cascadelake
        Frequency: 2.594 GHz 
        Logical CPU Count: 72

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.